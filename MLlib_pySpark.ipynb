{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b83e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('imbalanced_binary_classification').getOrCreate()\n",
    "new_df = spark.read.csv('application_train.csv', header=True, inferSchema=True)\n",
    "new_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008c7136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(new_df.take(10), columns= new_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d8289d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除SK_ID_CURR列\n",
    "drop_col = ['SK_ID_CURR']\n",
    "new_df = new_df.select([column for column in new_df.columns if column not in drop_col])\n",
    "new_df = new_df.withColumnRenamed('TARGET', 'label')\n",
    "new_df.groupby('label').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d000849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目标变量的分布情况\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline                    # 在Notebook中内联显示matplotlib绘图，而不是弹出一个新的窗口显示绘图结果。\n",
    "df_pd = new_df.toPandas()\n",
    "print(len(df_pd))\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.countplot(x='label', data=df_pd, order=df_pd['label'].value_counts().index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a877972b",
   "metadata": {},
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05117eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看有多少个categorical features和numerical features：\n",
    "\n",
    "cat_cols = [item[0] for item in new_df.dtypes if item[1].startswith('string')] \n",
    "print(str(len(cat_cols)) + '  categorical features')\n",
    "\n",
    "num_cols = [item[0] for item in new_df.dtypes if item[1].startswith('int') | item[1].startswith('double')][1:]\n",
    "print(str(len(num_cols)) + '  numerical features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8b6984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查找有关缺失值的更多信息\n",
    "\n",
    "def info_missing_table(df_pd):\n",
    "    \"\"\"输入 pandas 数据框并返回缺失值和百分比的列\"\"\"\n",
    "    mis_val = df_pd.isnull().sum()         #计算数据框中每列中空值的总数\n",
    "    mis_val_percent = 100 * df_pd.isnull().sum() / len(df_pd) #计算每列中空值的百分比\n",
    "    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)  #连接两个表\n",
    "    mis_val_table_ren_columns = mis_val_table.rename(\n",
    "    columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "    \n",
    "    # 从表格中筛选出具有缺失值的列（百分比不为0）      \n",
    "    mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "    mis_val_table_ren_columns.iloc[:,1] != 0].sort_values('% of Total Values', ascending=False).round(1)   \n",
    "    \n",
    "    print (\"Your selected dataframe has \" + str(df_pd.shape[1]) + \" columns.\\n\"    # 原表格有121列  \n",
    "    \"There are \" + str(mis_val_table_ren_columns.shape[0]) +              \n",
    "    \" columns that have missing values.\") # 含缺失值的有67列\n",
    "    return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0982d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "missings = info_missing_table(df_pd)\n",
    "missings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e6203c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理 Spark DataFrame，查找更多关于数据集中缺失值的情况\n",
    "\n",
    "def count_missings(spark_df):\n",
    "    null_counts = []        \n",
    "    for col in spark_df.dtypes:    \n",
    "        cname = col[0]     \n",
    "        ctype = col[1]      \n",
    "        nulls = spark_df.where( spark_df[cname].isNull()).count()  # 统计当前列中的空值数量\n",
    "        result = tuple([cname, nulls])  #new tuple, (列名, 空值数量)\n",
    "        null_counts.append(result)      # 将包含列名和空值数量的元组 `result` 添加到 `null_counts` 列表中\n",
    "    null_counts=[(x,y) for (x,y) in null_counts if y!=0]  # 只返回有缺失值的列的信息\n",
    "    return null_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4f34b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_counts = count_missings(new_df)\n",
    "miss_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c060bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据缺失值的列，将`new_df` 中的列分为 categorical type 和 numerical type\n",
    "\n",
    "list_cols_miss=[x[0] for x in miss_counts]       # 从之前计算的包含缺失值列名和缺失值数量的列表 `miss_counts` 中提取出列名\n",
    "df_miss= new_df.select(*list_cols_miss)         # 创建一个新的 DataFrame `df_miss`，该 DataFrame 只包含存在缺失值的列。\n",
    "\n",
    "#categorical columns\n",
    "catcolums_miss=[item[0] for item in df_miss.dtypes if item[1].startswith('string')] \n",
    "print(\"cateogrical columns_miss:\", catcolums_miss)\n",
    "\n",
    "### numerical columns\n",
    "numcolumns_miss = [item[0] for item in df_miss.dtypes if item[1].startswith('int') | item[1].startswith('double')] \n",
    "print(\"numerical columns_miss:\", numcolumns_miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4cee3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将每个 categotical 列中的缺失值填充为该列中的众数\n",
    "\n",
    "from pyspark.sql.functions import rank,sum,col\n",
    "df_Nomiss=new_df.na.drop()\n",
    "for x in catcolums_miss:\n",
    "    mode=df_Nomiss.groupBy(x).count().sort(col(\"count\").desc()).collect()[0][0]   # 计算每列的众数\n",
    "    print(x, mode)  # 列名和众数 \n",
    "    new_df = new_df.na.fill({x:mode})  # 填充缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271d395a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将每个 numerical 列中的缺失值填充为该列中的平均数\n",
    "\n",
    "from pyspark.sql.functions import mean, round\n",
    "\n",
    "for i in numcolumns_miss:\n",
    "    meanvalue = new_df.select(round(mean(i))).collect()[0][0] \n",
    "    print(i, meanvalue) \n",
    "    new_df=new_df.na.fill({i:meanvalue}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135ef1eb",
   "metadata": {},
   "source": [
    "## 处理不平衡类别：\n",
    "### 在这种情况下，我们在数据集中添加一个名为“权重”的新列，并用每个类别（1,0）的比例（0.91,0.09）填充它"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc5d7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 增加权重并填充比例\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "ratio = 0.91\n",
    "def weight_balance(labels):\n",
    "    return when(labels == 1, ratio).otherwise(1*(1-ratio))\n",
    "\n",
    "new_df = new_df.withColumn('weights', weight_balance(col('label')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceeb8089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看数据集中是否仍存在缺失值\n",
    "\n",
    "miss_counts2 = count_missings(new_df)\n",
    "miss_counts2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573ae85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(new_df.take(10), columns= new_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91efb53c",
   "metadata": {},
   "source": [
    "## 特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271eded9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 spark 中 MLlib 的 OneHotEncoderEstimator 将每个 categorical 特征转换为 one-hot 编码向量\n",
    "# 接下来，使用 VectorAssembler 将生成的 one-hot 向量和其余 numerical 特征组合成一个向量列。我们将流程的每一步附加到阶段数组中\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "\n",
    "stages = []\n",
    "for categoricalCol in cat_cols:\n",
    "    # 对 categorical 特征进行字符串索引\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n",
    "    \n",
    "    # 将索引后的特征进行 one-hot 编码\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    stages += [stringIndexer, encoder]\n",
    "\n",
    "# 组合所有的特征列\n",
    "assemblerInputs = [c + \"classVec\" for c in cat_cols] + num_cols            # 包含所有要组合的特征列的列表\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")    # 将所有特征组合成一个向量\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed574e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 pipeline 将 new_df 转换成一个包含所有预处理特征的 DataFrame\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "cols = new_df.columns\n",
    "pipeline = Pipeline(stages = stages) \n",
    "pipelineModel = pipeline.fit(new_df)         # 拟合模型\n",
    "new_df = pipelineModel.transform(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971ef4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedCols = ['features']+cols\n",
    "new_df = new_df.select(selectedCols)\n",
    "pd.DataFrame(new_df.take(5), columns=new_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b29d0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分割训练集和测试集\n",
    "\n",
    "train, test = new_df.randomSplit([0.80, 0.20], seed = 42)\n",
    "print(train.count())\n",
    "print(test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a5641f",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf0e048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用逻辑回归进行训练\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "LR = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=15)   # 初始化回归模型指定特征列，目标列以及最大迭代次数（15）\n",
    "LR_model = LR.fit(train) # 拟合模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df466ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制ROC曲线\n",
    "\n",
    "trainingSummary = LR_model.summary        # 获取模型训练后的摘要信息，其中包含模型性能的各种指标，如 ROC 曲线、精度、召回率等。\n",
    "\n",
    "roc = trainingSummary.roc.toPandas()\n",
    "plt.plot(roc['FPR'],roc['TPR'])           # 绘制曲线\n",
    "plt.ylabel('False Positive Rate')\n",
    "plt.xlabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "\n",
    "print('Training set ROC: ' + str(trainingSummary.areaUnderROC))     # 获取 ROC 曲线下面积（AUC）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82926a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查模型在测试集上的性能：\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "predictions_LR = LR_model.transform(test)      # 将训练好的逻辑回归模型 LR_model 应用于测试集\n",
    "evaluator = BinaryClassificationEvaluator()    # 创建一个 BinaryClassificationEvaluator 实例，用于评估二分类模型的性能。\n",
    "print(\"Test_SET Area Under ROC: \" + str(evaluator.evaluate(predictions_LR, {evaluator.metricName: \"areaUnderROC\"})))   # 打印 AUC 值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5139a758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBT\n",
    "\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "gbt = GBTClassifier(maxIter=15)                # 初始化 GBT 分类器并设置最大迭代次数\n",
    "GBT_Model = gbt.fit(train)                     # 拟合模型\n",
    "predictions = GBT_Model.transform(test)        # 将训练好的逻辑回归模型 GBT_Model 应用于测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b3b9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator()\n",
    "print(\"Test_SET Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaf7605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用网格搜索实现超参数调整，然后运行交叉验证以更好地提高 GBT 的性能\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# 创建参数网格\n",
    "paramGrid = (ParamGridBuilder()                     # 初始化参数网格构建器\n",
    "             .addGrid(gbt.maxDepth, [2, 4, 6])      # 设置最大树深度为 2、4、6，控制模型的复杂度\n",
    "             .addGrid(gbt.maxBins, [20, 30])        # 设置树分裂时每个特征的最大划分数为 20 和 30。这个参数影响连续特征的分箱处理。\n",
    "             .addGrid(gbt.maxIter, [10, 15])        # 设置最大迭代次数为 10 和 15，表示模型将构建的决策树数目。\n",
    "             .build())\n",
    "\n",
    "# 初始化交叉验证器\n",
    "cv = CrossValidator(estimator=gbt, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# 执行交叉验证\n",
    "cvModel = cv.fit(train)             # 验证并保存最佳模型\n",
    "predictions = cvModel.transform(test)     # 生成预测\n",
    "evaluator.evaluate(predictions)     # 评估性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e0729b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4482c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f953e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
